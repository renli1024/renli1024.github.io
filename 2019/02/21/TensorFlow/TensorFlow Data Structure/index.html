<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>TensorFlow Data Structure | Ren Li&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="Ren Li's Blog" />
  
  <meta name="description" content="本文介绍了TensorFlow的基本数据类型和使用方法。">
<meta name="keywords" content="TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow Data Structure">
<meta property="og:url" content="https://renli1024.github.io/2019/02/21/TensorFlow/TensorFlow Data Structure/index.html">
<meta property="og:site_name" content="Ren Li&#39;s blog">
<meta property="og:description" content="本文介绍了TensorFlow的基本数据类型和使用方法。">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-08-22T10:13:36.274Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow Data Structure">
<meta name="twitter:description" content="本文介绍了TensorFlow的基本数据类型和使用方法。">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  <script src="/js/pace.min.js"></script>
  

  
  

</head>

<body>
  <div id="container">
      <header id="header">
    <div id="header-outer">
        <div id="header-menu" class="header-menu-pos animated">
            <div class="header-menu-container">
                <a href="/" class="left">
                    <span class="site-title">Ren Li&#39;s Blog</span>
                </a>
                <nav id="header-menu-nav" class="right">
                    
                    <a  href="/">
                        <i class="fa fa-home"></i>
                        <span>Home</span>
                    </a>
                    
                    <a  href="/archives">
                        <i class="fa fa-archive"></i>
                        <span>Archives</span>
                    </a>
                    
                    <a  href="https://github.com/lrStyle">
                        <i class="fa fa-user"></i>
                        <span>Github</span>
                    </a>
                    
                </nav>
                <a class="mobile-header-menu-button">
                    <i class="fa fa-bars"></i>
                </a>
            </div>
        </div>
    </div>
</header>
      <div class="outer">
        <section id="main" class="body-wrap"><article id="post-TensorFlow/TensorFlow Data Structure" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="post-title" itemprop="name">
      TensorFlow Data Structure
    </h1>
    <div class="post-title-bar">
      <ul>
          
        <li>
          <i class="fa fa-calendar"></i>  2019-02-21
        </li>
        
      </ul>
    </div>
  

          
      </header>
    
    <div class="article-entry post-content" itemprop="articleBody">
      
            
            <p>本文介绍了TensorFlow的基本数据类型和使用方法。<br><a id="more"></a><br>本文中<code>tf</code>都指代的是<code>TensorFlow</code></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>TensorFlow中可以认为只有两种基本数据结构：tensor 和 operation。operation代表一个操作，对应计算图中的节点，也可以认为是<code>函数</code>；tensor本质是一个句柄，代表的是operation的计算结果，也可认为是<code>函数的返回值</code>。</li>
</ul>
<h2 id="tf-Tensor"><a href="#tf-Tensor" class="headerlink" title="tf.Tensor"></a>tf.Tensor</h2><ul>
<li>tensor的中文意思是“张量”，可将其理解为高维向量，向量是一维tensor，矩阵是二维tensor。</li>
<li>Tensor底层的值是由<code>numpy.array</code>来实现的，因此两者间会有很多共通之处。</li>
<li>关于tensor和op的关系：A Tensor is a symbolic handle to one of the outputs of an Operation。即tensor本质是一个句柄，绑定着op的结果，因此一个tensor必然会有一个operation（但一个op可以输出多个tensor），所以tensor中才会有<code>Tensor.op</code>属性，表示是什么op产生了这个tensor。</li>
<li>也正因为tensor是句柄，并不存储值，只存储计算结果，所以只有经过计算，tensor才会有相应的值，因此只有运行Session后才能得出tensor的值。</li>
</ul>
<h3 id="tensor命名规则"><a href="#tensor命名规则" class="headerlink" title="tensor命名规则"></a>tensor命名规则</h3><ul>
<li>tensor的命名格式为<code>&lt;op_name&gt;:&lt;output_index&gt;</code></li>
<li>对返回tensor的API，会自动创建op，并通过<code>name</code>参数指定的操作的名字（即<code>op_name</code>）；如果后续有相同命名的操作，则在<code>op_name</code>后加<code>_1</code>；对于其产生的tensor，则按产生顺序自动在后面加<code>:0/:1/:2</code>等。</li>
<li>一般<code>op_name</code>命名为大写开头。</li>
<li>example code：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">W1 = tf.Variable([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]],dtype = tf.float32,name=<span class="string">'Var'</span>) </span><br><span class="line">W2 = tf.Variable([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]],dtype = tf.float32,name=<span class="string">'Var'</span>)</span><br><span class="line">A1 = tf.add(W1, W2)</span><br><span class="line">B1, B2 = tf.split(W1, <span class="number">2</span>)</span><br><span class="line">C1 = tf.constant(value=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], name=<span class="string">"Con1"</span>)</span><br><span class="line">P1 = tf.placeholder(tf.int32, shape=(<span class="number">2</span>,<span class="number">3</span>), name=<span class="string">"Pla1"</span>)</span><br><span class="line">print(W1)</span><br><span class="line">print(W2)</span><br><span class="line">print(A1)</span><br><span class="line">print(B1)</span><br><span class="line">print(B2)</span><br><span class="line">print(C1)</span><br><span class="line">print(P1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;tf.Variable 'Var:0' shape=(2, 3) dtype=float32_ref&gt;</span></span><br><span class="line"><span class="comment"># &lt;tf.Variable 'Var_1:0' shape=(2, 3) dtype=float32_ref&gt;</span></span><br><span class="line"><span class="comment"># Tensor("Add:0", shape=(2, 3), dtype=float32)</span></span><br><span class="line"><span class="comment"># Tensor("split:0", shape=(1, 3), dtype=float32)</span></span><br><span class="line"><span class="comment"># Tensor("split:1", shape=(1, 3), dtype=float32)</span></span><br><span class="line"><span class="comment"># Tensor("Con1:0", shape=(3,), dtype=int32)</span></span><br><span class="line"><span class="comment"># Tensor("Pla1:0", shape=(2, 3), dtype=int32</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Rank-of-Tensors"><a href="#Rank-of-Tensors" class="headerlink" title="Rank of Tensors"></a>Rank of Tensors</h3><p>rank指tensor的秩，即维数。不同rank的tensor会有不同的数学意义：</p>
<table>
<thead>
<tr>
<th style="text-align:center">rank</th>
<th style="text-align:left">数学意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:left">Scalar(标量)</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:left">Vector(向量)</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:left">Matrix(矩阵)</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:left">Cube(立方体)</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:left">一行Cube（立方体再向高维抽象）</td>
</tr>
</tbody>
</table>
<ul>
<li>如何获得tensor的rank：因为其不是Tensor的属性之一，所以无法直接访问对象获得，只能通过<code>tf.rank(some_Tensor)</code>方法来获取维数（返回的一个封装了rank值的tensor）</li>
<li>访问tensor中的某个scalar：每一维指定对应的标号即可。<br>eg. <code>my_scalar = my_matrix[1, 2]</code>, <code>my_column_vector = my_matrix[:, 3]</code>。其中<code>:</code>表示取这一维度中的所有数。</li>
</ul>
<h3 id="Shape-of-Tensors"><a href="#Shape-of-Tensors" class="headerlink" title="Shape of Tensors"></a>Shape of Tensors</h3><ul>
<li>shape表示tenor各维度的长度，或者说各维度中元素的个数。<br>eg. <code>tf.constant([[[1., 2., 3.]], [[7., 8., 9.]]])</code>：rank为3，shape为[2, 1, 3]。</li>
<li>tensor的shape可以只指定一部分，如<code>TensorShape([None, 256])</code>，只指定了列数为256列，行数会在运行时自动被推断出。</li>
<li>如何获得tensor的shape：<ol>
<li>通过<code>tf.Tensor.shape</code>属性（推荐）：返回一个<code>TensorShape</code>对象，通过slice机制来访问封装在<code>TensorShape</code>中各个维度的长度；</li>
<li>通过<code>tf.shape(my_tensor)</code>方法：返回一个<code>Tensor</code>对象，表示原tensor的shape。因为是tensor类型，所以必须运行后才能知道其具体值。但好处因为是在运行时计算的，所以此时shape一定是fully-defined的。</li>
</ol>
</li>
<li>如何改变tensor的shape：通过<code>tf.reshape()</code>方法。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rank_three_tensor = tf.ones([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">matrix = tf.reshape(rank_three_tensor, [<span class="number">6</span>, <span class="number">10</span>]) </span><br><span class="line"><span class="comment"># Reshape existing content into a 6x10 matrix</span></span><br><span class="line"></span><br><span class="line">matrixB = tf.reshape(matrix, [<span class="number">3</span>, <span class="number">-1</span>]) </span><br><span class="line"><span class="comment"># Reshape existing content into a 3x20 matrix. </span></span><br><span class="line"><span class="comment"># -1 tells reshape to calculate the size of this dimension.</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Data-Types-of-Tensors"><a href="#Data-Types-of-Tensors" class="headerlink" title="Data Types of Tensors"></a>Data Types of Tensors</h3><ul>
<li>每个<code>tf.tensor</code>都有一个<code>dtype</code>属性，表示tensor中存储的数据的数据类型，通过<code>tf.tensor.dtype</code>访问。</li>
<li>常用数据类型有：<code>tf.int32</code>（对应python中的整型）, <code>tf.float32</code>（对应python中的浮点型）。</li>
<li>如何转换tensor类型：<code>float_tensor = tf.cast(tf.constant([1, 2, 3]), dtype=tf.float32)</code></li>
</ul>
<h3 id="如何得到tensor的值"><a href="#如何得到tensor的值" class="headerlink" title="如何得到tensor的值"></a>如何得到tensor的值</h3><ul>
<li>通过<code>tf.Tensor.eval()</code>方法，它其实是<code>tf.get_default_session().run(tf.Tensor)</code>方法的简写。</li>
<li>注1：eval()方法必须在默认的session被激活的情况下才可执行（通过<code>with tf.Session().as_default():</code>语句块）。</li>
<li>注2：eval方法同样可以传入<code>feed_dict</code>，如<code>t.eval(feed_dict={p:2.0})</code></li>
</ul>
<h2 id="tf-Variable"><a href="#tf-Variable" class="headerlink" title="tf.Variable"></a>tf.Variable</h2><ul>
<li><code>tf.Variable</code>为tensorflow中的变量，值可以通过附加在其上的操作更改。</li>
<li>声明一个Variable后，系统会生成四个相关操作：<code>v_name/initial_value</code>, <code>v_name</code>, <code>v_name/Assign</code>, <code>v_name/read</code>, tf即通过这四个op实现对Variable值的更改。</li>
</ul>
<h3 id="tensor和variable的关系"><a href="#tensor和variable的关系" class="headerlink" title="tensor和variable的关系"></a>tensor和variable的关系</h3><ul>
<li>variable对应<code>tf.Variable</code>类型，tensor应该对应的是<code>tf.Tensor</code>。从内部实现上讲，variable本质是对tensor的一种封装。</li>
<li>TensorFlow官方将<code>tf.Variable</code>定义为<code>Tensor-like objects</code>，即类tensor类型（其他还有<code>numpy.ndarray</code>, <code>list</code>以及其他python基本类型: <code>bool, float, int, str</code>），这些类型在传入函数时会被隐式转换为tensor类型（using <code>tf.convert_to_tensor()</code> method），因此也可以当作tensor来使用。</li>
<li><code>tf.Variable</code>实现的意义在于提供一种值可以更改的类tensor类型。因为tensor本身实质上属于<strong>常量</strong>，值一旦初始化是不可更改的（<code>tf.constant()</code>、<code>tf.placeholder()</code>都只是产生tensor的方法，返回值是tensor)。</li>
<li>所以如果要在graph中训练参数，那么参数只能是variable形式（tensor不能改变值），tensor则用来表示输入、输出值，在graph中传递中间的计算值。</li>
</ul>
<h3 id="定义Variables"><a href="#定义Variables" class="headerlink" title="定义Variables"></a>定义Variables</h3><ul>
<li>官方推荐使用<code>tf.get_variable()</code>函数来定义、初始化以及调用变量，配合<code>tf.variable_scope</code>来确保不会不小心重用或声明新的变量。</li>
<li>就不要用tf.Variable()。</li>
<li><code>tf.get_variable()</code>方法原型：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(</span><br><span class="line">    name,</span><br><span class="line">    shape=<span class="keyword">None</span>,</span><br><span class="line">    dtype=<span class="keyword">None</span>,</span><br><span class="line">    initializer=<span class="keyword">None</span>,</span><br><span class="line">    regularizer=<span class="keyword">None</span>,</span><br><span class="line">    trainable=<span class="keyword">None</span>,</span><br><span class="line">    collections=<span class="keyword">None</span>,</span><br><span class="line">    caching_device=<span class="keyword">None</span>,</span><br><span class="line">    partitioner=<span class="keyword">None</span>,</span><br><span class="line">    validate_shape=<span class="keyword">True</span>,</span><br><span class="line">    use_resource=<span class="keyword">None</span>,</span><br><span class="line">    custom_getter=<span class="keyword">None</span>,</span><br><span class="line">    constraint=<span class="keyword">None</span>,</span><br><span class="line">    synchronization=tf.VariableSynchronization.AUTO,</span><br><span class="line">    aggregation=tf.VariableAggregation.NONE</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>其主要有两个作用：</p>
<ol>
<li>计算图中不存在name参数的值时，会声明一个变量，等价于调用variable的构造函数<code>tf.Variable()</code>；</li>
<li>计算图中已存在name参数时，返回对应的变量，因此可通过这个方法来根据名字获取变量；</li>
</ol>
<ul>
<li>详细参数说明：<br>name：必填参数，用于指定变量的名字；<br>shape：指定变量的shape；<br>initializer：指定变量的初始器，默认为<code>tf.glorot_uniform_initializer</code>。<br>collections：指定变量属于的集合，默认为<code>GraphKeys.GLOBAL_VARIABLES</code>。</li>
</ul>
<h3 id="初始化variable"><a href="#初始化variable" class="headerlink" title="初始化variable"></a>初始化variable</h3><p>variable在使用前必须初始化，初始化变量有两步：</p>
<ol>
<li>在声明变量时指定<code>initializer</code>参数，其可以是一个<br><code>tf.initializer</code>对象，也可以是一个tensor。若是initializer则会按该initializer的方式来初始化，若是tensor则会用该tensor的值来初始化。默认初始器参数为<code>tf.glorot_uniform_initializer</code>。</li>
<li>调用<code>tf.global_variables_initializer()</code>方法，对<code>tf.GraphKeys.GLOBAL_VARIABLES</code>集合中的变量进行初始化（<br>按变量指定的<code>initializer</code>参数进行初始化）。</li>
</ol>
<h3 id="使用variable"><a href="#使用variable" class="headerlink" title="使用variable"></a>使用variable</h3><ul>
<li><p>因为variable是tensor-like类型，所以直接像普通tensor一样使用variable即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v = tf.get_variable(<span class="string">"v"</span>, shape=(), initializer=tf.zeros_initializer())</span><br><span class="line">w = v + <span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>如何为variable重新分配值：使用<code>tf.Variable.assign()</code>、<code>tf.Variable.assign_add()</code>、<code>tf.Variable.assign_sub()</code>三个方法，可用对Variable重新赋值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = tf.get_variable(<span class="string">"v"</span>, shape=(), initializer=tf.zeros_initializer())</span><br><span class="line">assignment = v.assign_add(<span class="number">1</span>) <span class="comment"># assign方法会返回一个新分配值的tensor</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line">sess.run(assignment)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Variable-collections"><a href="#Variable-collections" class="headerlink" title="Variable collections"></a>Variable collections</h3><p>tf提供了collection机制，使得变量的存储和访问变得很方便。系统常用collections：</p>
<ul>
<li><code>tf.GraphKeys.GLOBAL_VARIABLES</code>：全局共享的变量（每个设备都可以访问），变量默认都存储在这个集合中。</li>
<li><code>tf.GraphKeys.TRAINABLE_VARIABLES</code>：存储会被训练的变量，<code>tf.Optimizer</code>就是优化此集合下的变量。<br>注：所有变量默认都是存储在<code>TRAINABLE_VARIABLES</code>和<code>GLOBAL_VARIABLES</code>这两个集合中。<code>TRAINABLE_VARIABLES</code>属于<code>GLOBAL_VARIABLES</code>的一个子集，两者并不冲突。</li>
<li><code>tf.GraphKeys.LOCAL_VARIABLES</code>：用于存储临时变量，在这个集合中的变量不会被优化。</li>
<li>如何将变量添加到指定集合中：<ol>
<li>在声明变量的时候指定<code>collections</code>参数</li>
<li>通过调用<code>tf.add_to_collection(collection_name, variable_name)</code>函数。</li>
</ol>
</li>
</ul>
<h3 id="tf-variable-scope机制"><a href="#tf-variable-scope机制" class="headerlink" title="tf.variable_scope机制"></a>tf.variable_scope机制</h3><p><code>tf.variable_scope</code>的作用有两点：</p>
<ol>
<li>对变量使用范围加以限制，以方便变量的create和share，在一个scope中构造的变量就只能在这个scope中使用。</li>
<li>起到<code>tf.name_scope</code>的作用，对变量命名进行整理。</li>
</ol>
<ul>
<li><code>reuse = True</code>：总是重用变量（变量没有的话会出错），<code>reuse = tf.AUTO_REUSE</code>：如果变量没有会新建一个变量，有的话则重用。</li>
<li><p>example code：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer1"</span>):</span><br><span class="line">        tf.get_variable(name=<span class="string">'Var'</span>, shape=[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># "layer1/Var"</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer2"</span>):</span><br><span class="line">        tf.get_variable(name=<span class="string">'Var'</span>, shape=[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 会重新定义一个"layer2/Var"变量，而非重用之前的变量</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>如果确实希望重用相同的内部变量，可通过如下方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example 1</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"layer1"</span>):</span><br><span class="line">    V1 = tf.get_variable(name=<span class="string">'Var'</span>, shape=[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"layer1"</span>):</span><br><span class="line">    V2 = tf.get_variable(name=<span class="string">'Var'</span>) <span class="comment"># 报错，禁止重用变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># example 2</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"layer1"</span>):</span><br><span class="line">    V1 = tf.get_variable(name=<span class="string">'Var'</span>, shape=[<span class="number">1</span>])</span><br><span class="line">    V2 = tf.get_variable(name=<span class="string">'Var'</span>) <span class="comment"># 报错，禁止重用变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># example 3</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"layer1"</span>):</span><br><span class="line">    V1 = tf.get_variable(name=<span class="string">'Var'</span>, shape=[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># "layer1/Var"</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"layer1"</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    V2 = tf.get_variable(name=<span class="string">'Var'</span>)</span><br><span class="line">    <span class="comment"># 正确, V1, V2重用同一个变量</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="tf-variable-scope和tf-name-scope的区别"><a href="#tf-variable-scope和tf-name-scope的区别" class="headerlink" title="tf.variable_scope和tf.name_scope的区别"></a>tf.variable_scope和tf.name_scope的区别</h3><ul>
<li>name_scope：仅仅是为了更好地管理变量的命名空间而提出的，对变量的使用没有任何影响。此外tensorboard会将一个name_scope下的变量整理在一个父节点中，因此良好的name scope命名规则会很有利于模型可视化。</li>
<li>variable_scope：不仅仅是管理命名空间，还是为了限制变量的使用范围，常和 tf.get_variable()配合使用，来实现变量共享的功能。tensorboard也会对同一variable_scope下的变量进行整理。</li>
<li>可以认为variable_scope是name_scope针对variable的“加强版”。</li>
</ul>
<h2 id="tf-constant"><a href="#tf-constant" class="headerlink" title="tf.constant"></a>tf.constant</h2><p><code>tf.constant</code>是常量tensor，其在声明的时候就定义好值了，且值不可改变。<br>eg. <code>tf.constant(3.0, dtype=tf.float32)</code></p>
<h2 id="tf-placeholder"><a href="#tf-placeholder" class="headerlink" title="tf.placeholder"></a>tf.placeholder</h2><p>占位符，声明的时候只需要指定类型即可，在运算的时候再指定值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32)</span><br><span class="line">y = tf.placeholder(tf.float32)</span><br><span class="line">z = x + y</span><br><span class="line">print(sess.run(z, feed_dict=&#123;x: <span class="number">3</span>, y: <span class="number">4.5</span>&#125;))</span><br><span class="line">print(sess.run(z, feed_dict=&#123;x: [<span class="number">1</span>, <span class="number">3</span>], y: [<span class="number">2</span>, <span class="number">4</span>]&#125;))</span><br></pre></td></tr></table></figure></p>
<ul>
<li>注：placeholder不能eval()，其值只能通过feed_dict的形式传入。</li>
<li>feed_dict：将值以字典的形式传入placeholder，字典的键可以为：<ul>
<li>placeholder的变量名，直接传入变量名；</li>
<li>placeholder的<code>name</code>属性，字符串类型，但必须满足<code>&lt;op_name&gt;:&lt;output_index&gt;</code>的格式（tf中tensor的命名规范），否则系统无法将其对应到相应的placeholder上。</li>
</ul>
</li>
</ul>

            <div class="post-copyright">
    <hr />
    <div class="content">
        <p>Post Date： 2019-02-21</p>
        <p>版权声明：&nbsp;本文为原创文章，转载请注明出处</p>
    </div>
</div>

      
        
            

        
    </div>
    <footer class="article-footer">
        
        <ul class="article-footer-menu">
            
            
  <li class="article-footer-tags">
    <i class="fa fa-tags"></i>
      
    <a href="/tags/TensorFlow/" class="color1">TensorFlow</a>
      
  </li>

        </ul>
        
    </footer>
  </div>
</article>


    <aside class="post-toc-pos post-toc-top" id="post-toc">
        <nav class="post-toc-wrap">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Introduction"><span class="post-toc-number">1.</span> <span class="post-toc-text">Introduction</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tf-Tensor"><span class="post-toc-number">2.</span> <span class="post-toc-text">tf.Tensor</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#tensor命名规则"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">tensor命名规则</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Rank-of-Tensors"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">Rank of Tensors</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Shape-of-Tensors"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">Shape of Tensors</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Data-Types-of-Tensors"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">Data Types of Tensors</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#如何得到tensor的值"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">如何得到tensor的值</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tf-Variable"><span class="post-toc-number">3.</span> <span class="post-toc-text">tf.Variable</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#tensor和variable的关系"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">tensor和variable的关系</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#定义Variables"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">定义Variables</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#初始化variable"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">初始化variable</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#使用variable"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">使用variable</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Variable-collections"><span class="post-toc-number">3.5.</span> <span class="post-toc-text">Variable collections</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#tf-variable-scope机制"><span class="post-toc-number">3.6.</span> <span class="post-toc-text">tf.variable_scope机制</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#tf-variable-scope和tf-name-scope的区别"><span class="post-toc-number">3.7.</span> <span class="post-toc-text">tf.variable_scope和tf.name_scope的区别</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tf-constant"><span class="post-toc-number">4.</span> <span class="post-toc-text">tf.constant</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tf-placeholder"><span class="post-toc-number">5.</span> <span class="post-toc-text">tf.placeholder</span></a></li></ol>
        </nav>
    </aside>
    



    
</section>
        
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info" class="inner">
      <p>
        Powered by  <a href="http://hexo.io/" target="_blank">Hexo</a>
        Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a>
      &copy; 2019 Ren Li<br>
      </p>
    </div>
  </div>
</footer>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script>
  var mihoConfig = {
      root: "https://renli1024.github.io",
      animate: false,
      isHome: false,
      share: false,
      reward: 1
  }
</script>
<div class="sidebar">
    <div id="sidebar-search" title="Search">
        <i class="fa fa-search"></i>
    </div>
    <div id="sidebar-category" title="Categories">
        <i class="fa fa-book"></i>
    </div>
    <div id="sidebar-tag" title="Tags">
        <i class="fa fa-tags"></i>
    </div>
    <div id="sidebar-top">
        <span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span>
    </div>
</div>
<div class="sidebar-menu-box" id="sidebar-menu-box">
    <div class="sidebar-menu-box-container">
        <div id="sidebar-menu-box-categories">
            
        </div>
        <div id="sidebar-menu-box-tags">
            <a href="/tags/Algorithm/" style="font-size: 12.86px;">Algorithm</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/C-Sharp/" style="font-size: 10px;">C_Sharp</a> <a href="/tags/English/" style="font-size: 14.29px;">English</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/JSP/" style="font-size: 10px;">JSP</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Lingo/" style="font-size: 10px;">Lingo</a> <a href="/tags/Linux/" style="font-size: 11.43px;">Linux</a> <a href="/tags/Mac/" style="font-size: 11.43px;">Mac</a> <a href="/tags/MachineLearning/" style="font-size: 10px;">MachineLearning</a> <a href="/tags/Mathematics/" style="font-size: 11.43px;">Mathematics</a> <a href="/tags/Matlab/" style="font-size: 11.43px;">Matlab</a> <a href="/tags/Network/" style="font-size: 15.71px;">Network</a> <a href="/tags/Other/" style="font-size: 20px;">Other</a> <a href="/tags/Paper-Notes/" style="font-size: 14.29px;">Paper Notes</a> <a href="/tags/Python/" style="font-size: 17.14px;">Python</a> <a href="/tags/SQL/" style="font-size: 10px;">SQL</a> <a href="/tags/Swift/" style="font-size: 10px;">Swift</a> <a href="/tags/TensorFlow/" style="font-size: 18.57px;">TensorFlow</a> <a href="/tags/cs224n/" style="font-size: 15.71px;">cs224n</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a> <a href="/tags/正则表达式/" style="font-size: 10px;">正则表达式</a> <a href="/tags/课程笔记/" style="font-size: 10px;">课程笔记</a>
        </div>
    </div>
    <a href="javascript:;" class="sidebar-menu-box-close">&times;</a>
</div>
<div class="mobile-header-menu-nav" id="mobile-header-menu-nav">
    <div class="mobile-header-menu-container">
        <span class="title">Menus</span>
        <ul class="mobile-header-menu-navbar">
            
            <li>
                <a  href="/">
                    <i class="fa fa-home"></i><span>Home</span>
                </a>
            </li>
            
            <li>
                <a  href="/archives">
                    <i class="fa fa-archive"></i><span>Archives</span>
                </a>
            </li>
            
            <li>
                <a  href="https://github.com/lrStyle">
                    <i class="fa fa-user"></i><span>Github</span>
                </a>
            </li>
            
        </ul>
    </div>
    <div class="mobile-header-tag-container">
        <span class="title">Tags</span>
        <div id="mobile-header-container-tags">
            <a href="/tags/Algorithm/" style="font-size: 12.86px;">Algorithm</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/C-Sharp/" style="font-size: 10px;">C_Sharp</a> <a href="/tags/English/" style="font-size: 14.29px;">English</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/JSP/" style="font-size: 10px;">JSP</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Lingo/" style="font-size: 10px;">Lingo</a> <a href="/tags/Linux/" style="font-size: 11.43px;">Linux</a> <a href="/tags/Mac/" style="font-size: 11.43px;">Mac</a> <a href="/tags/MachineLearning/" style="font-size: 10px;">MachineLearning</a> <a href="/tags/Mathematics/" style="font-size: 11.43px;">Mathematics</a> <a href="/tags/Matlab/" style="font-size: 11.43px;">Matlab</a> <a href="/tags/Network/" style="font-size: 15.71px;">Network</a> <a href="/tags/Other/" style="font-size: 20px;">Other</a> <a href="/tags/Paper-Notes/" style="font-size: 14.29px;">Paper Notes</a> <a href="/tags/Python/" style="font-size: 17.14px;">Python</a> <a href="/tags/SQL/" style="font-size: 10px;">SQL</a> <a href="/tags/Swift/" style="font-size: 10px;">Swift</a> <a href="/tags/TensorFlow/" style="font-size: 18.57px;">TensorFlow</a> <a href="/tags/cs224n/" style="font-size: 15.71px;">cs224n</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a> <a href="/tags/正则表达式/" style="font-size: 10px;">正则表达式</a> <a href="/tags/课程笔记/" style="font-size: 10px;">课程笔记</a>
        </div>
    </div>
</div>
<div class="search-wrap">
    <span class="search-close">&times;</span>
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
            <i class="icon icon-lg icon-chevron-left"></i>
        </a>
        <input class="search-field" placeholder="Search..." id="keywords">
        <a id="search-submit" href="javascript:;">
            <i class="fa fa-search"></i>
        </a>
    <div class="search-container" id="search-container">
        <ul class="search-result" id="search-result">
        </ul>
    </div>
</div>

<div id="search-tpl">
    <li class="search-result-item">
        <a href="{url}" class="search-item-li">
            <span class="search-item-li-title" title="{title}">{title}</span>
        </a>
    </li>
</div>
<script src="/js/search.js"></script>
<script src="/js/main.js"></script>









  <script src="/js/pop-img.js"></script>
  <script>
     $(".article-entry p img").popImg();
  </script>

  </div>
</body>
</html>